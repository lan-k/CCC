{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "udr6M4ZAJB2r"
      },
      "source": [
        "![alt text](https://drive.google.com/uc?export=view&id=1-_5W9fdhR8mk7NrULAHOGK7stXHKmAdb)\n",
        "\n",
        "---\n",
        "<br>\n",
        "© 2022 Copyright The University of New South Wales - CRICOS 00098G\n",
        "\n",
        "**Author**: Oscar Perez-Concha: o.perezconcha@unsw.edu.au\n",
        "\n",
        "**Contributors/Co-authors**: Marta Fredes-Torres, Zhisheng (Sandy) Sa and Matthew Sainsbury-Dale.\n",
        "\n",
        "\n",
        "# Linear Prediction\n",
        "# Exercise: Diabetes Hospitalisations - Logistic Regression with L1-norm and L2-norm Regularization (Lasso and Ridge) \n",
        "\n",
        "\n",
        "# 1. Introduction\n",
        "Following the visualisation and manipulation stages (data mining and machine learning work-flow), we are now going to fit/build a predictive model.\n",
        "[Online resources](http://scikit-learn.org/stable/tutorial/basic/tutorial.html)\n",
        "\n",
        "<b>Goal/Research question:</b> <font color=green> <b> our final goal is to build a predictive algorithm to predict readmission to hospital 30 days after discharge. </b></font>\n",
        "\n",
        "\n",
        "\n",
        "## 1.1. Regression with L1-norm Regularization (Lasso) and L2-norm Regularization (Ridge)\n",
        "\n",
        "In this exercise, we are going to fit a logistic regression to our data set, by using two techniques that constrains (or regularizes) the coefficient estimates with a L1-norm (Lasso regularization) and a L2-norm regularization (Ridge regularization). \n",
        "\n",
        "1. The Lasso regularization will shrink the coefficient estimates towards zero or <b>directly to zero</b>. For this last reason, Lasso regularization is a technique for feature selection (those features that are not zero in the model). \n",
        "\n",
        "2. The Ridge regularization will shrink the coefficient estimates **towards zero**.\n",
        "\n",
        "The advantage of regularization versus plain least squares has to do with the bias-variance trade off. For more information, read the book [An Introduction to Statistical Learning with Applications in R by Gareth James, Daniela Witten, Trevor Hastie and Robert Tibshirani](http://www-bcf.usc.edu/~gareth/ISL/ISLR%20Seventh%20Printing.pdf).\n",
        "\n",
        "\n",
        "## 1.2. Aims:\n",
        " 1. To build a predictive model <b>using logistic regression with L1-norm and L2-norm regularization of the coefficient estimates</b> (Lasso and Ridge regularization).\n",
        " 2. To choose input variables. \n",
        " 3. To manipulate features: standardization.\n",
        " 4. To use training and test sets.\n",
        " 5. To continue becoming familiar with the diabetes inpatient hospital dataset and the clinical terms contained in it.\n",
        " \n",
        "It aligns with all the learning outcomes of our course: \n",
        "\n",
        "\n",
        "1. Distinguish a range of task specific machine learning techniques appropriate for Health Data Science.\n",
        "2. Design machine learning tasks for Health Data Science scenarios.\n",
        "3. Apply machine learning workflow to health data problems.\n",
        "4. Generate knowledge via the application of machine learning techniques to health data.\n",
        "\n",
        "\n",
        "## 1.3. Jupyter Notebook Intructions\n",
        "1. Read the content of each cell.\n",
        "2. Where necessary, follow the instructions that are written in each cell.\n",
        "3. Run/Execute all the cells that contain Python code sequentially (one at a time), using the \"Run\" button.\n",
        "4. For those cells in which you are asked to write some code, please write the Python code first and then execute/run the cell.\n",
        " \n",
        "## 1.4. Tips\n",
        "1. Run all the cells in sequence (one at a time), using the \"Run\" button.\n",
        "2. To edit this notebook, just double-click in each cell. Choose between \"Code\" cell or text \"Markdown\" cell in the combo-box above. \n",
        "3. If you want to save your notebook, please make sure you press \"the floppy disk\" icon button above. \n",
        "4. To clean the content of all cells and re-start Notebook, please go to Cell->All Output->Clear\n",
        "\n",
        "Follow the instructions given and if you have any questions, please use the **Comments section** in **Open Learning**. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NR-BjOUgJB2v"
      },
      "source": [
        "# 2. Docstring: \n",
        "\n",
        "Create a docstring with the variables and constants that you will use in this exercise (data dictionary) and the purpose of your program. It is expected that you choose informative variable names and document your program (both docstrings and comments)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ckez68PkJB2w"
      },
      "source": [
        "<b> Write the answer here:</b>\n",
        "#####################################################################################################################\n",
        "\n",
        "(double-click here)\n",
        "\n",
        "\n",
        "#####################################################################################################################"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B0YATiOuJB2y"
      },
      "source": [
        "# 3. Load the dataset stored in pickle (from Exercise 1)\n",
        "\n",
        "**Let's load the dataset that we prepared in the previous exercise of this Chapter.** Please note that we stored the dataset using pickle. Now, we will load this dataset, using 'pickle' ([more information](https://docs.python.org/3/library/pickle.html))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "YB9PgWXFJdn2",
        "outputId": "b07bbfe3-9196-45ba-b35b-78d1cb5ccee6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Installing:  {'lime', 'grid', 'shap'}\n"
          ]
        }
      ],
      "source": [
        "# check required libraries are installed if not calling system to install\n",
        "import sys\n",
        "import subprocess\n",
        "import pkg_resources\n",
        "\n",
        "required = {'numpy', 'pandas', 'plotnine', 'matplotlib', 'seaborn', \n",
        "            'grid', 'lime', 'shap', 'scikit-learn'}\n",
        "installed = {pkg.key for pkg in pkg_resources.working_set}\n",
        "missing = required - installed\n",
        "\n",
        "if missing:\n",
        "    print('Installing: ', missing)\n",
        "    python = sys.executable\n",
        "    subprocess.check_call([python, '-m', 'pip', 'install', *missing], stdout=subprocess.DEVNULL)\n",
        "# delete unwanted variables\n",
        "del required \n",
        "del installed \n",
        "del missing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "C0BOxQXiJB2z"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from IPython.display import display"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "ChT_v4xJJjlk",
        "outputId": "a235c810-867c-41a0-dc64-6bef31c1206a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "# Mount Google Drive\n",
        "# We do not need to run this cell if you are not running this notebook in Google Colab\n",
        "\n",
        "if 'google.colab' in str(get_ipython()):\n",
        "    from google.colab import drive # import drive from Gogle colab\n",
        "    root = '/content/drive'     # default location for the drive\n",
        "    # print(root)                 # print content of ROOT (Optional)\n",
        "    drive.mount(root)\n",
        "else:\n",
        "    print('Not running on CoLab')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4FZnzabPkv1_"
      },
      "source": [
        "If you are running this notebook in Google Colab, you must define your project paths. In this case, define your `project_path`. Otherwise, all the data will be lost after you close the session."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "dP6NwfR2KN-Q"
      },
      "outputs": [],
      "source": [
        "from pathlib import Path\n",
        "\n",
        "if 'google.colab' in str(get_ipython()):\n",
        "    # EDIT THE PROJECT PATH IF DIFFERENT WITH YOUR ONE\n",
        "    project_path = Path(root) / 'MyDrive' / 'HDAT9500' / 'chapter01'\n",
        "\n",
        "    # OPTIONAL - set working directory according to your google drive project path\n",
        "    # import os\n",
        "    # Change directory to the location defined in project_path\n",
        "    # os.chdir(project_path)\n",
        "else:\n",
        "    project_path = Path()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "aZwmIcjiJB21",
        "outputId": "6225f873-999a-47b9-9dab-9ae45b7337c5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 317
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   los  Age  number_diagnoses  num_lab_procedures  num_procedures  \\\n",
              "0    2   79                 9                  38               0   \n",
              "1    5   59                 8                  49               0   \n",
              "2    2   33                 5                  62               0   \n",
              "3    6   42                 9                  77               0   \n",
              "4    1   62                 7                  13               5   \n",
              "\n",
              "   num_medications  number_emergency  number_inpatient  number_outpatient  \\\n",
              "0               12                 0                 0                  0   \n",
              "1               16                 0                 0                  0   \n",
              "2               15                 1                 0                  1   \n",
              "3               30                 0                 0                  0   \n",
              "4                6                 0                 0                  0   \n",
              "\n",
              "   sex_Female  ...  admission_source_grouped_Emergency Room  \\\n",
              "0           1  ...                                        1   \n",
              "1           0  ...                                        1   \n",
              "2           1  ...                                        1   \n",
              "3           1  ...                                        1   \n",
              "4           0  ...                                        1   \n",
              "\n",
              "   admission_source_grouped_Other  \\\n",
              "0                               0   \n",
              "1                               0   \n",
              "2                               0   \n",
              "3                               0   \n",
              "4                               0   \n",
              "\n",
              "   admission_source_grouped_Physician Referral  \\\n",
              "0                                            0   \n",
              "1                                            0   \n",
              "2                                            0   \n",
              "3                                            0   \n",
              "4                                            0   \n",
              "\n",
              "   admission_source_grouped_Transfer from another health care facility  \\\n",
              "0                                                  0                     \n",
              "1                                                  0                     \n",
              "2                                                  0                     \n",
              "3                                                  0                     \n",
              "4                                                  0                     \n",
              "\n",
              "   admission_type_grouped_Elective  admission_type_grouped_Emergency  \\\n",
              "0                                1                                 0   \n",
              "1                                1                                 0   \n",
              "2                                1                                 0   \n",
              "3                                1                                 0   \n",
              "4                                1                                 0   \n",
              "\n",
              "   admission_type_grouped_Not Available/Null  \\\n",
              "0                                          0   \n",
              "1                                          0   \n",
              "2                                          0   \n",
              "3                                          0   \n",
              "4                                          0   \n",
              "\n",
              "   admission_type_grouped_Trauma Centre  admission_type_grouped_Urgent  \\\n",
              "0                                     0                              0   \n",
              "1                                     0                              0   \n",
              "2                                     0                              0   \n",
              "3                                     0                              0   \n",
              "4                                     0                              0   \n",
              "\n",
              "   readmission  \n",
              "0           no  \n",
              "1           no  \n",
              "2           no  \n",
              "3           no  \n",
              "4           no  \n",
              "\n",
              "[5 rows x 64 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-c16d793b-d202-4aa4-a9f6-5d09df9cfb08\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>los</th>\n",
              "      <th>Age</th>\n",
              "      <th>number_diagnoses</th>\n",
              "      <th>num_lab_procedures</th>\n",
              "      <th>num_procedures</th>\n",
              "      <th>num_medications</th>\n",
              "      <th>number_emergency</th>\n",
              "      <th>number_inpatient</th>\n",
              "      <th>number_outpatient</th>\n",
              "      <th>sex_Female</th>\n",
              "      <th>...</th>\n",
              "      <th>admission_source_grouped_Emergency Room</th>\n",
              "      <th>admission_source_grouped_Other</th>\n",
              "      <th>admission_source_grouped_Physician Referral</th>\n",
              "      <th>admission_source_grouped_Transfer from another health care facility</th>\n",
              "      <th>admission_type_grouped_Elective</th>\n",
              "      <th>admission_type_grouped_Emergency</th>\n",
              "      <th>admission_type_grouped_Not Available/Null</th>\n",
              "      <th>admission_type_grouped_Trauma Centre</th>\n",
              "      <th>admission_type_grouped_Urgent</th>\n",
              "      <th>readmission</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2</td>\n",
              "      <td>79</td>\n",
              "      <td>9</td>\n",
              "      <td>38</td>\n",
              "      <td>0</td>\n",
              "      <td>12</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>...</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>no</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>5</td>\n",
              "      <td>59</td>\n",
              "      <td>8</td>\n",
              "      <td>49</td>\n",
              "      <td>0</td>\n",
              "      <td>16</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>no</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>33</td>\n",
              "      <td>5</td>\n",
              "      <td>62</td>\n",
              "      <td>0</td>\n",
              "      <td>15</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>...</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>no</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>6</td>\n",
              "      <td>42</td>\n",
              "      <td>9</td>\n",
              "      <td>77</td>\n",
              "      <td>0</td>\n",
              "      <td>30</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>...</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>no</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1</td>\n",
              "      <td>62</td>\n",
              "      <td>7</td>\n",
              "      <td>13</td>\n",
              "      <td>5</td>\n",
              "      <td>6</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>no</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 64 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-c16d793b-d202-4aa4-a9f6-5d09df9cfb08')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-c16d793b-d202-4aa4-a9f6-5d09df9cfb08 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-c16d793b-d202-4aa4-a9f6-5d09df9cfb08');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ],
      "source": [
        "import pickle\n",
        "#pickle_path = Path(project_path) /'hospital_data.pickle' \n",
        "pickle_path = '/content/drive/My Drive/Colab Notebooks/hospital_data.pickle'\n",
        "# Load dataset stored in pickle in Exercise 1\n",
        "with open(pickle_path, 'rb') as data:\n",
        "    hospital = pickle.load(data)\n",
        "    \n",
        "hospital.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "DEeNplcWJB22",
        "outputId": "f01663bf-acff-487c-b49a-fa616fa7e03e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Index(['los', 'Age', 'number_diagnoses', 'num_lab_procedures',\n",
              "       'num_procedures', 'num_medications', 'number_emergency',\n",
              "       'number_inpatient', 'number_outpatient', 'sex_Female', 'sex_Male',\n",
              "       'max_glu_serum_>200', 'max_glu_serum_>300', 'max_glu_serum_None',\n",
              "       'max_glu_serum_Norm', 'A1Cresult_>7', 'A1Cresult_>8', 'A1Cresult_None',\n",
              "       'A1Cresult_Norm', 'group_name_1_Blood_&_immune', 'group_name_1_CNS',\n",
              "       'group_name_1_Cancer', 'group_name_1_Cardiac_&_circulatory',\n",
              "       'group_name_1_Digestive', 'group_name_1_Endocrine',\n",
              "       'group_name_1_Infectious', 'group_name_1_Mental_&_Substance',\n",
              "       'group_name_1_Other', 'group_name_1_Respiratory',\n",
              "       'group_name_2_Blood_&_immune', 'group_name_2_CNS',\n",
              "       'group_name_2_Cancer', 'group_name_2_Cardiac_&_circulatory',\n",
              "       'group_name_2_Digestive', 'group_name_2_Endocrine',\n",
              "       'group_name_2_Infectious', 'group_name_2_Mental_&_Substance',\n",
              "       'group_name_2_Other', 'group_name_2_Respiratory',\n",
              "       'group_name_3_Blood_&_immune', 'group_name_3_CNS',\n",
              "       'group_name_3_Cancer', 'group_name_3_Cardiac_&_circulatory',\n",
              "       'group_name_3_Digestive', 'group_name_3_Endocrine',\n",
              "       'group_name_3_Infectious', 'group_name_3_Mental_&_Substance',\n",
              "       'group_name_3_Other', 'group_name_3_Respiratory',\n",
              "       'discharge_disposition_grouped_Discharged to home',\n",
              "       'discharge_disposition_grouped_Home health service',\n",
              "       'discharge_disposition_grouped_Other',\n",
              "       'discharge_disposition_grouped_Short term hospital',\n",
              "       'discharge_disposition_grouped_Transferred to SNF',\n",
              "       'admission_source_grouped_Emergency Room',\n",
              "       'admission_source_grouped_Other',\n",
              "       'admission_source_grouped_Physician Referral',\n",
              "       'admission_source_grouped_Transfer from another health care facility',\n",
              "       'admission_type_grouped_Elective', 'admission_type_grouped_Emergency',\n",
              "       'admission_type_grouped_Not Available/Null',\n",
              "       'admission_type_grouped_Trauma Centre', 'admission_type_grouped_Urgent',\n",
              "       'readmission'],\n",
              "      dtype='object')"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ],
      "source": [
        "hospital.columns"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pvQfG1ZmJB23"
      },
      "source": [
        "# 4. Feature scaling: standardizing the features"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lZuiNdldJB24"
      },
      "source": [
        "For some of the methods covered in this chapter, specifically ridge and lasso regression, it is best to standardize the features (see book  [An Introduction to Statistical Learning with Applications in R by Gareth James, Daniela Witten, Trevor Hastie and Robert Tibshirani](http://www-bcf.usc.edu/~gareth/ISL/ISLR%20Seventh%20Printing.pdf))."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eKH3JcTDJB24"
      },
      "source": [
        " **To standardize a feature**, subtract the sample mean, $\\bar x$, and then divide by its estimated standard deviation, $s$. This process results in the standardized features all having a standard deviation of 1 and a mean of 0. This makes each feature to have the same shape and spread, so that no single feature dominates the regression based purely on its scaling and distribution.<p> \n",
        "\n",
        "If you want to read a bit more about this issue, I leave some links here:\n",
        "\n",
        "1. Read the section related to this topic (it used to be page 217, I am not sure if it has changed), from the book  [An Introduction to Statistical Learning with Applications in R by Gareth James, Daniela Witten, Trevor Hastie and Robert Tibshirani](http://www-bcf.usc.edu/~gareth/ISL/ISLR%20Seventh%20Printing.pdf).\n",
        "\n",
        "    This the mathematical formula for standardizing features as per the book \"Tibshirani: \n",
        "    \n",
        " ![alt text](https://drive.google.com/uc?export=view&id=1539bqr47y_vWM1iONl2kuNUtFPcXvGrK)  \n",
        "    \n",
        "    \n",
        "2. [Towards Data Science](https://towardsdatascience.com/scale-standardize-or-normalize-with-scikit-learn-6ccc7d176a02)\n",
        "3. [One of the forums in which the ML community is very active](https://stats.stackexchange.com/questions/290958/logistic-regression-and-scaling-of-features)\n",
        " \n",
        "        \n",
        "Let's standardize our features:\n",
        "[`StandardScaler`](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MwGvje81JB25"
      },
      "source": [
        "**Interpretation of beta coefficients**: \n",
        "\n",
        "Keep in mind that the interpretation of regression coefficients becomes more complicated after the features are standardized. For instance, consider multiple linear regression. Prior to standardization, a unit change in $X_j$ corresponds to a change of $\\hat \\beta_j$ to the predicted response. Simple. However, after standardization of $X_j$, the interpretation of the coefficients becomes: for every increase of <b>one standard deviation</b> in $X_j$, the predicted response changes by $\\hat \\beta_j$ standard deviations. Still manageable, but not as straight forward.<p>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U6si5c38JB26"
      },
      "source": [
        "First, we split the hospital data set into two DataFrames: the features, stored in X, and the target, stored in y."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eNb5NqksJB27"
      },
      "outputs": [],
      "source": [
        "X = hospital.drop(['readmission'], axis = 1)\n",
        "y = hospital[['readmission']].values"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9lzlmxD7JB28"
      },
      "source": [
        "Now let's split the data into a training and test set. We will include the optional argument 'stratify = y' to preserve the ratio between readmission = NO to readmission = YES."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LsJ7q7tXJB29"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Gzn4y7jmJB29"
      },
      "outputs": [],
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X, y, stratify = y, test_size=0.25,random_state=0)\n",
        "# stratify = y means we wish to preserve the ratio of y in the test and training sets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CUb-Q6G5JB2-"
      },
      "outputs": [],
      "source": [
        "# Sanity Check:\n",
        "\n",
        "# Check that proportions are maintained:\n",
        "y_test_df = pd.DataFrame(y_test)\n",
        "y_test_df.columns = ['readmission']\n",
        "y_train_df = pd.DataFrame(y_train)\n",
        "y_train_df.columns = ['readmission']\n",
        "\n",
        "print('*********************************************************************************')\n",
        "print(f\"\"\"Number of records in original data frame (hospital): {len(y)}, \n",
        "Proportion of readmission in original data frame (hospital):\\n {hospital.readmission.value_counts('NO')}\\n\n",
        "********************************************************************************* \\n\n",
        "Number of records in test set: {len(y_test)}, \n",
        "Proportion of readmission in test set:\\n {y_test_df.readmission.value_counts('NO')}\\n\n",
        "********************************************************************************* \\n\n",
        "Number of records in training set: {len(y_train)}, \n",
        "Proportion of readmission in train set:\\n {y_train_df.readmission.value_counts('NO')}\n",
        "\"\"\")\n",
        "print('*********************************************************************************')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "haLWd7luJB2_"
      },
      "source": [
        "We can see that the training and test set have maintained the proportion of NO:YES response from the original data.  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9wMxrL9YJB2_"
      },
      "source": [
        "**Returning to the process of standardization. Now, we need to make a choice as to whether to standardize the dummy variables, or not.**<p>  \n",
        "    According to Robert Tibrishani, the author of the lasso method,  \"*The lasso* (and ridge) *method requires initial standardization of the regressors, so that the penalization scheme is fair to all regressors. For categorical regressors, one codes the regressor with dummy variables and then <b> standardizes the dummy variables </b>*\".<p>\n",
        "        \n",
        "If you want to read a bit more about why we should standarize our variables, I leave you here a couple of links:\n",
        "\n",
        "1. Paper written by Robert Tibshirani of Stanford University, one of the creators of Ridge and Lasso (click [here](https://onlinelibrary.wiley.com/doi/pdf/10.1002/(SICI)1097-0258(19970228)16:4%3C385::AID-SIM380%3E3.0.CO;2-3) and read for the paragraph I wrote above). I include the paper in the data folder of the GitHub too.\n",
        "2. [A forum discussion](https://stats.stackexchange.com/questions/69568/whether-to-rescale-indicator-binary-dummy-predictors-for-lasso/120600)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mMwzN6iJJB2_"
      },
      "source": [
        "Now let's define the scaler we will use, which we decide is `StandardScaler`. `StandardScaler` performs the standardization described at the beginning of this section. We can also easily replace one preprocessing algorithm with another by simply changing the scaler definition at this step. For instance, instead of using `StandardScaler`, we could have used `MinMaxScaler`, or some other alternative."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ty8K_fX1JB2_"
      },
      "source": [
        "[More information](http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "YjtDhlWzJB3A",
        "outputId": "11ebde7f-edcd-4e9e-a7d5-daedeec45ad1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 168
        }
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-804eb3fc9ae7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mscaler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mStandardScaler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'StandardScaler' is not defined"
          ]
        }
      ],
      "source": [
        "scaler = StandardScaler()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7CqykzCZJB3A"
      },
      "source": [
        "### <font color='blue'> <b>Very very very very important!:</b></font>\n",
        "\n",
        "<font color='blue'> Now we fit the scaler to the training set only. It is important that we fit the scaler to **the training set only** and then apply that same scaler to the test set (remember that the test set is never seen until the very end, once our model has been fully trained). \n",
        "    \n",
        "The purpose of having a test set is to mimic the situation where your model is making prediction decisions in the real world, when you do not have access to the true response. Therefore, we cannot use the test set for *anything* except comparing to predicted values. We treat the test as \"the future data\" that we will receive once the algorithm has been trained. This means we cannot use the test set to fit the scaler as the test set is considered \"future data\".</font>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9cNuFtRMJB3A"
      },
      "source": [
        "Standardize features means to subtract the mean and divide by the standard deviation.\n",
        "In this step, we calculate the actual means and variances for each feature in  <font color='red'> THE TRAINING SET.</font>\n",
        "\n",
        "[`StandardScaler`](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ty2bexWLJB3B"
      },
      "outputs": [],
      "source": [
        "scaler.fit(X_train)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "drbJw4poJB3B"
      },
      "source": [
        "To actually scale the data, use the transform method."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xnK3uXoyJB3C"
      },
      "outputs": [],
      "source": [
        "# rescale the training data\n",
        "X_train_scaled = scaler.transform(X_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O_CiIKq0JB3C"
      },
      "outputs": [],
      "source": [
        "# Check\n",
        "X_train_scaled"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NZFOqwbwJB3D"
      },
      "source": [
        "To apply predictive models to the scaled data, we also need to transform the test set."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ht3ODjyvJB3D"
      },
      "outputs": [],
      "source": [
        "# scale the test data \n",
        "X_test_scaled = scaler.transform(X_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Dsl4TouTJB3E"
      },
      "outputs": [],
      "source": [
        "# Check \n",
        "X_test_scaled"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2FdSJTulJB3E"
      },
      "source": [
        "# 5. Logistic Regression"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PVMNbgpKJB3F"
      },
      "source": [
        "Logistic regression is used for *binary classification*, which means the response, $y$,  is a categorical variable with 2 levels. This is precisely the situation for modelling <b>readmission</b>, which has levels YES and NO. Now, rather than modelling the response variable directly, logistic regression involves modelling the probability that the response variable belongs to a particular category. Logistic regression estimates the probability by the **logistic function**:<p>\n",
        "\n",
        "\\begin{align*}\n",
        " P(Y=1 | X) \\ &= \\frac{e^{\\beta_0 + \\beta_1 X_1 + \\cdots + \\beta_p X_p}}{1+e^{\\beta_0 + \\beta_1 X_1 + \\cdots + \\beta_p X_p}} \\\\\n",
        "            \\ &= \\frac{1}{1+e^{-(\\beta_0 + \\beta_1 X_1 + \\cdots + \\beta_p X_p)}} \n",
        "\\end{align*}\n",
        "\n",
        "    \n",
        "\n",
        "Now, the reason we model the probability via the logistic equation is to 'squash' the output of the model to the range (0,1). This is required as probabilities must be between 0 and 1 (there cannot be a negative chance of occurrence and there cannot be a greater than certain chance of occurrence).<p>\n",
        "   \n",
        "## Logit Function and Log Odds\n",
        "    \n",
        "        \n",
        "We can also rearrange the logistic function into what is known as the **logit function**:<p>\n",
        "    \\begin{equation*}\n",
        "log\\left(\\frac{P(y=YES)}{1-P(y=YES)}\\right) = \\beta_0 + \\beta_1 X_1 + \\cdots + \\beta_p X_p \n",
        "\\end{equation*}<p>\n",
        "    Now, the quantity $\\frac{P(y=YES)}{1-P(y=YES)}$ has a special interpretation. It is the *odds ratio* of an event occuring, and can take on any value between $0$ and $\\infty$. Notice that $1-P(y=YES)$ is the probability that y is *not* YES, so the odds ratio is the probability that $y = YES$ over the probability that $y\\ne YES$. This shows that logistic regression essentially involves modelling the *log odds* of an event via a linear regression model. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tPKTUWxoJB3F"
      },
      "source": [
        "## Likelihood Function\n",
        "\n",
        "Remember that we encode YES as 1, and NO as 0. <p>\n",
        "    Now we will go a little bit further. The way in which the model is fitted is more complicated than linear regression. For logistic regression, we choose the beta coefficients by the method of **maximum likelihood**. Intuitively, we try to find values for the beta coefficients such that the probability is close to 1 when the true value is YES, and close to 0 when the true value is NO. We can express this mathematically as trying to *maximise* the **likelihood function**:<p>\n",
        "      \n",
        "\n",
        "    \n",
        "\\begin{align*}\n",
        " L(\\beta|X) \\ &= \\prod_{i: y_i = 1}{P(y_i = 1)}\\prod_{i': y_i' = 0}{P(y_i=0)} \\\\\n",
        "            \\ &= \\prod_{i: y_i = 1}{P(y_i = 1)}\\prod_{i': y_i' = 0}{[1 - P(y_i = 1)]} \n",
        "\\end{align*}\n",
        "\n",
        "    \n",
        "Don't be intimidated by this. Let's unpack: the $\\prod$ sign just means 'multiply everything together', similar to how the familiar $\\sum$ operates. Below the first $\\prod$, we see '$i: y_i = 1$', which means 'do the multiplying for every i such that $y_i = 1$'. Similarly, below the second $\\prod$ we see '$i: y_i = 0$', which means 'do the multiplying for every i such that $y_i = 0$'. Finally, $P(y_i = 1)$ means 'the probability that $y_i = YES$.<p>\n",
        "        \n",
        "The details are not so important, so don't worry too much if you are not familiar with this notation. To maximise the likelihood, we use the convenient property that maximising the likelihood is equivalent to maximising the logarithm of the likelihood, which converts the above products into sums. This is helpful as sums are easier to maximise via differentiation methods. In addition, it is convenient to code the two-class via a 0/1 response $y_i$.\n",
        "The **log-likelihood** can be written:<p>\n",
        "\n",
        "\\begin{align*}\n",
        " \\textit {l} (\\beta|X)) \\ &= log(L(\\beta|X)) \\\\\n",
        " \\ &= \\sum_{i: y_i = 1}{log(P(y_i = 1))} + \\sum_{i: y_i = 0}{log(1-P(y_i=1))} \\\\\n",
        "            \\ &= \\sum_{i}^{N} {y_i log(P(y_i = 1))} + \\sum_{i}^N{(1-y_i)log(1-P(y_i=1))} \\\\\n",
        "            \\ &= \\sum_{i}^{N} {\\left[y_i log(P(y_i = 1))+ (1-y_i)log(1-P(y_i=1))\\right]} \\\\\n",
        "\\end{align*}\n",
        "\n",
        "\n",
        "Recall that $P(y=1) = \\frac{1}{1+e^{-(\\beta_0 + \\beta_1 X_1 + \\cdots + \\beta_p X_p)}}$. Then the log-likelihood function becomes:<p>\n",
        "\n",
        "\\begin{align*}\n",
        "\\textit {l} (\\beta|X)) \\ &=  \\sum_{i}^{N} {\\left[y_i log(\\frac{1}{1+e^{-(\\beta_0 + \\beta_1 X_1 + \\cdots + \\beta_p X_p)}})+ (1-y_i)log(1-\\frac{1}{1+e^{-(\\beta_0 + \\beta_1 X_1 + \\cdots + \\beta_p X_p)}})\\right]} \\\\\n",
        "\\end{align*}\n",
        "\n",
        "Now, we simply maximize this log-likehood expression to find the betas. How do we do that? Very simple, set the first derivative equal to zero. We will leave the full explanation for a more advanced machine learning course, but it is good to know how this *maximum likelihood principle* works because many machine learning algorithms use this principle.<p>\n",
        "    So, in summary, regular logistic regression chooses beta coefficients by maximising the log-likelihood function, $l(\\beta)$."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hwU1tPfFJB3G"
      },
      "source": [
        "## L2 Regularization: Ridge \n",
        "\n",
        "Now, L2 regularization, also known as Ridge Regression, shrinks the beta coefficients by imposing a penalty on their size. This is achieved by adding a penalty term to the likelihood function maximised by regular logistic regression. The strength of the penalty is commonly denoted as C in the [`sklearn.linear_model.LogisticRegression`](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html) function. For the sake of clarity, this term is also known as $\\lambda$, $\\alpha$, or$\\frac{1}{C}$ and $(\\frac {1}{\\lambda}  = \\frac {1}{\\alpha} = C)$.\n",
        "\n",
        " ![alt text](https://drive.google.com/uc?export=view&id=1qkonZrzLiq6rVfluBWfUbad5ixRL93hx)  \n",
        " \n",
        "For more information and to see the original images, read page 215 of the book [An Introduction to Statistical Learning with Applications in R by Gareth James, Daniela Witten, Trevor Hastie and Robert Tibshirani](http://www-bcf.usc.edu/~gareth/ISL/ISLR%20Seventh%20Printing.pdf))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xd7I-x-cJB3G"
      },
      "source": [
        "## L1 Regularization: Lasso \n",
        "\n",
        "Similarly to L2 regularization, lasso regression shrinks the beta coefficients by adding a penalty term to the likelihood function optimised by regular logistic regression. The difference between the two regularizations is the form of this penalty term. For Ridge, we have penalty term as the beta coefficients *squared*. For Lasso, we have the *absolute value*. Again, the strength of the penalty is commonly denoted as C in the [`sklearn.linear_model.LogisticRegression`](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html) function. For the sake of clarity, this term is also known as $\\lambda$, $\\alpha$, or$\\frac{1}{C}$ and $(\\frac {1}{\\lambda}  = \\frac {1}{\\alpha} = C)$.\n",
        "\n",
        " ![alt text](https://drive.google.com/uc?export=view&id=1_0CoRRnzjWXLaI-Otwcx8wqRQN06-Bxv)  \n",
        " \n",
        "For more information and to see the original images, read page 219 of the book [An Introduction to Statistical Learning with Applications in R by Gareth James, Daniela Witten, Trevor Hastie and Robert Tibshirani](http://www-bcf.usc.edu/~gareth/ISL/ISLR%20Seventh%20Printing.pdf))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ce9n05lbJB3G"
      },
      "source": [
        "## 5.1 Training a logistic regression model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "idCP1LryJB3G"
      },
      "source": [
        "Now we can train our logistic regression model using ridge regression. The default of logistic regression in <b>scikit-learn</b>  is to include a ridge penalty term in the linear component. The strength of the penalty is commonly denoted as ${C}$ in the [`sklearn.linear_model.LogisticRegression`](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html) function. For the sake of clarity: $(\\frac {1}{\\lambda}  = \\frac {1}{\\alpha} = C)$.\n",
        "    \n",
        "    Small values of $C$ indicate strong regularization, whilst larger values of $C$ indicate weaker regularization. In fact, if we set $C$ to be an extremely large number, then $\\lambda$ is effectively zero and the regression reduces to <b> regular logistic regression</b> . "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QahnPGeSJB3G"
      },
      "outputs": [],
      "source": [
        "from sklearn.linear_model import LogisticRegression"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yT9gDxEtJB3H"
      },
      "source": [
        "[More information on `LogisticRegression`](http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D_kTiai3JB3H"
      },
      "source": [
        "![alt text](https://drive.google.com/uc?export=view&id=1xCsYAddYioVlNoFY9Vi4dZEi-ODnycof)  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nTuDy0isJB3H"
      },
      "source": [
        "Let's use the `liblinear` solver that is valid for L1 and L2 regularizations."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZMTwtpkNJB3H"
      },
      "source": [
        "### Training a logistic regression model using L1-norm regularization (Lasso)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6GIJPRv7JB3I"
      },
      "outputs": [],
      "source": [
        "# C=0.01 as a first step\n",
        "Log_Reg_L1 = LogisticRegression(C = 0.01 , penalty = 'l1', solver='liblinear').fit(X_train_scaled, y_train.ravel()) \n",
        "# ravel() used to convert y from column vector to 1d array, as required by the method"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2V0Xs_VkJB3I"
      },
      "source": [
        "### Training a logistic regression model using L2-norm regularization (Ridge)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pAoGnRk_JB3I"
      },
      "outputs": [],
      "source": [
        "# C=0.01 as a first step\n",
        "Log_Reg_L2 = LogisticRegression(C = 0.01 , penalty = 'l2',solver='liblinear').fit(X_train_scaled, y_train.ravel()) \n",
        "# ravel() used to convert y from column vector to 1d array, as required by the method"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZsMxNqnJJB3J"
      },
      "source": [
        "<div class=\"alert alert-block alert-success\">**Start Activity 2**</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-yaTqHAhJB3J"
      },
      "source": [
        "### <font color='brown'> Question 1: Print the beta coefficients of both logistic regression models and comment on your first impression. Change the C hyperparameter several times and print the beta coefficients again to see how they change.</font>\n",
        "\n",
        "<font color='green'> Tip:  You can use the following structure: `ModelName.coef_` to print the beta coefficients only. \n",
        "You can use the following structure: `coefficients = pd.concat([pd.DataFrame(X.columns,columns=['Features']), pd.DataFrame(np.transpose(ModelName.coef_),columns=['Coefficients'])], axis = 1)` to print the beta coefficients of each variable from the diabetes dataset.</font>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "txX_U01YJB3J",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "# Write Python code here:\n",
        "\n",
        "# Log_Reg_L1.coef_  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "raJtbHCYJB3K",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "# Write Python code here:\n",
        "\n",
        "# Log_Reg_L2.coef_           "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8Kyd701dJB3L"
      },
      "source": [
        "<b> Write the answer here:</b>\n",
        "#####################################################################################################################\n",
        "\n",
        "\n",
        "#####################################################################################################################"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JB3zhP92JB3L"
      },
      "source": [
        "<div class=\"alert alert-block alert-warning\">**End Activity 2**</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3PxV-5tbJB3L"
      },
      "source": [
        "**This highlights the important difference between Ridge and Lasso. In Ridge, the coefficients are reduced, but not completely to zero. In Lasso, we have many of the coefficients becoming zero. This is a form of feature selection.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QrRvSlnTJB3L"
      },
      "source": [
        "## 5.2 Making predictions and using 'accuracy' to evaluate the model. Performance Measures."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZuREIpTbJB3M"
      },
      "source": [
        "We now have two model that can predict readmission based on the feature variables. Let's predict the first record in the test set:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z8JS6wPfJB3M"
      },
      "source": [
        "<div class=\"alert alert-block alert-success\">**Start Activity 3**</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7K3QC0jRJB3N"
      },
      "source": [
        "### <font color='brown'> Question 2: Write the Python code to make predictions using the test set with the model that we fitted previously. Make sure you save the predictions as 'y_pred', as this variable will be used later on.</font>\n",
        "\n",
        "<font color='green'> Tip:  You can use the following structure: `y_pred_Model1 = ModelName.predict(X_test_scaled)`</font>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "335JqiryJB3N"
      },
      "outputs": [],
      "source": [
        "# Write Python code here for \"LASSO\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CBOW3sdMJB3O"
      },
      "outputs": [],
      "source": [
        "# Write Python code here for \"RIDGE\"\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lg9Vb_jUJB3Q"
      },
      "source": [
        "<div class=\"alert alert-block alert-warning\">**End Activity 3**</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5pJ2WKErkv2Y"
      },
      "source": [
        "It is recommendable to read the section about \"Performance Measures\" in the textbook \"Hands-On Machine Learning with Scikit-Learn and TensorFlow: Concepts, Tools, and Techniques to Build Intelligent Systems Géron, Aurélien Sebastopol: O'Reilly Media, Incorporated, 2019\". \n",
        "\n",
        "In particular, you can read about Confusion Matrix, Precision and Recall, Precision/Recall Trade-off, The ROC Curve, \n",
        "and Multiclass Classification. Don't read about cross validation yet, as we will go through it in the next chapter."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PUSdv7JbJB3Q"
      },
      "source": [
        "<div class=\"alert alert-block alert-success\">**Start Activity 4**</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1pBWl4yIJB3Q"
      },
      "source": [
        "### <font color='brown'> Question 3: Compute the accuracy of both models 'Log_Reg_L1' and 'Log_Reg_L2', and give your opinion about it</font>\n",
        "\n",
        "<font color='green'> Tip:  You can use the following structure: `round(ModelName.score(X_test_scaled,y_test),3)`</font>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6IriYIZJJB3R"
      },
      "outputs": [],
      "source": [
        "# Write Python code here for \"LASSO\"\n",
        "# Use score method to get accuracy of model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_SW9OeQqJB3S"
      },
      "outputs": [],
      "source": [
        "# Write Python code here for \"RIDGE\"\n",
        "# Use score method to get accuracy of model\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LfR9Kfo3JB3T"
      },
      "source": [
        "<b> Write the answer here:</b>\n",
        "#####################################################################################################################\n",
        "\n",
        "(Double-click here)\n",
        "\n",
        "\n",
        "#####################################################################################################################"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CgFo6G1aJB3T"
      },
      "source": [
        "<div class=\"alert alert-block alert-warning\">**End Activity 4**</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VUhR__mZJB3U"
      },
      "source": [
        "Does the accuracy look like a good resut? But be careful..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VTiD5K9PJB3W"
      },
      "outputs": [],
      "source": [
        "print(hospital.readmission.value_counts('no'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C64-hh8sJB3W"
      },
      "source": [
        "We know that readmission = NO accounts for most of the records in our data. This means that if we set a model to always predict NO, we will automatically obtain good model accuracy! This highlights the flaw in using accuracy as the only performance metric, particularly for response variables that are imbalanced (many more of one level than the other). Let's look at the confusion matrix."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hrju2y4cJB3W"
      },
      "source": [
        "### 5.2.1 Confusion Matrix"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c4D4-dzSJB3X"
      },
      "outputs": [],
      "source": [
        "from sklearn import metrics\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VTIwFXL0JB3X"
      },
      "source": [
        "![alt text](https://drive.google.com/uc?export=view&id=1CD8LhgfkLhxOF83w6oU3TyqJwAEAIueb)  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L-8YqR3hJB3X"
      },
      "outputs": [],
      "source": [
        "# Confusion matrix for model using 'LASSO'\n",
        "\n",
        "# Confusion Matrix\n",
        "confusion_L1 = metrics.confusion_matrix(y_test, y_pred_L1)\n",
        "\n",
        "# Visualising the confusion matrix of our KNN model\n",
        "labels = {'No', 'Yes'}\n",
        "ax= plt.subplot()\n",
        "sns.heatmap(confusion_L1, annot=True, fmt='.0f', ax= ax, cmap=\"viridis\")\n",
        "\n",
        "# labels, title and ticks\n",
        "ax.set_xlabel('Predicted labels');ax.set_ylabel('True labels'); \n",
        "ax.set_title('Confusion Matrix'); \n",
        "ax.xaxis.set_ticklabels(['No', 'Yes']); ax.yaxis.set_ticklabels(['No', 'Yes'])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5ub3SCNfJB3Z"
      },
      "outputs": [],
      "source": [
        "# Confusion matrix for model using 'RIDGE'\n",
        "\n",
        "confusion_L2 = metrics.confusion_matrix(y_test, y_pred_L2)\n",
        "\n",
        "# Visualising the confusion matrix of our KNN model\n",
        "labels = {'No', 'Yes'}\n",
        "ax= plt.subplot()\n",
        "sns.heatmap(confusion_L2, annot=True, fmt='.0f', ax= ax, cmap=\"viridis\")\n",
        "\n",
        "# labels, title and ticks\n",
        "ax.set_xlabel('Predicted labels');ax.set_ylabel('True labels'); \n",
        "ax.set_title('Confusion Matrix'); \n",
        "ax.xaxis.set_ticklabels(['No', 'Yes']); ax.yaxis.set_ticklabels(['No', 'Yes'])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-8fAcytsJB3Z"
      },
      "source": [
        "The confusion matrix shows that only very few records were classified as YES, and almost all of the records were classified as NO. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kYBhF6ZvJB3Z"
      },
      "source": [
        "<div class=\"alert alert-block alert-success\">**Start Activity 5**</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DXubBpXbJB3a"
      },
      "source": [
        "### <font color='brown'> Question 4: Explain if you see any differences between both confusion matrix in your own words</font>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CxZGLl4NJB3b"
      },
      "source": [
        "<b> Write the answer here:</b>\n",
        "#####################################################################################################################\n",
        "\n",
        "(Double-click here)\n",
        "\n",
        "\n",
        "#####################################################################################################################"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HL_1b4ZAJB3b"
      },
      "source": [
        "<div class=\"alert alert-block alert-warning\">**End Activity 5**</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5Nz2-0XUJB3c"
      },
      "source": [
        "## 5.3 Evaluating the model using F1 Score - L1-norm (LASSO)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HPr6nxrrJB3c"
      },
      "source": [
        "In order to assess our model with a more suitable measure, we use the F1 score.\n",
        "\n",
        "[This article](https://machinelearningmastery.com/classification-accuracy-is-not-enough-more-performance-measures-you-can-use/) is a very nice reading about evaluation metrics or performance measures\n",
        "\n",
        "\n",
        "F1 scores need 'YES' to be 1 and 'NO' to be 0."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9mR4e8cZJB3c"
      },
      "source": [
        "We build a new list in which 'YES' will be 1 and 'NO' will be 0:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R-XSPYzrJB3d"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import f1_score\n",
        "round(f1_score(y_test, y_pred_L1, pos_label='yes', average='binary'),3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FCDSVHW4JB3d"
      },
      "source": [
        "The F1 score reaches its best value at 1 (perfect precision and recall) and worst at 0. As we can see, our predictive algorithm is not bad."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d5Mn9p_NJB3d"
      },
      "source": [
        "<div class=\"alert alert-block alert-success\">**Start Activity 6**</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KhImrD2LJB3e"
      },
      "source": [
        "### <font color='brown'> Question 5: Repeat the process of evaluating the model using F1 Score, but for our logistic regression model using L2-norm regularization (RIDGE) </font>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l4Kx94SKJB3e"
      },
      "outputs": [],
      "source": [
        "# Write Python code here.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0R1wVn4YJB3e"
      },
      "source": [
        "### <font color='brown'> Question 6: Use the 'classification_report' to list precision, recall, f1 score and support for each class. Repeat the process for both models.</font>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kIvApvrKJB3e"
      },
      "outputs": [],
      "source": [
        "# Write Python code here. TIP: Use y_pred_binary_L1 for LASSO\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2kcZetrDJB3e"
      },
      "outputs": [],
      "source": [
        "# Write Python code here. TIP: Use y_pred_binary_L2 for RIDGE\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V7-uA0dmJB3f"
      },
      "source": [
        "<div class=\"alert alert-block alert-warning\">**End Activity 6**</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_Xtf-fqYJB3f"
      },
      "source": [
        "## 5.4 Receiver Operating Characteristic (ROC): TPR and FPR\n",
        "\n",
        "[Information on `roc_curve`](http://scikit-learn.org/stable/modules/generated/sklearn.metrics.roc_curve.html)\n",
        "\n",
        "If you haven't seen [this video](https://www.youtube.com/watch?v=xugjARegisk) that explains ROC and AUC, I recommend it to you since this activity will be much clearer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RHb8AmEfJB3f"
      },
      "source": [
        "### 5.4.1 Probability associated with each prediction\n",
        "We need to determine the probability of each record in the test set being a 'YES', or equivalently a 1 as we have converted the response into a binary variable."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nemdJvuqJB3g"
      },
      "outputs": [],
      "source": [
        "# Probabilities of the test set being 0 and 1: LASSO\n",
        "y_pred_proba_L1 = Log_Reg_L1.predict_proba(X_test_scaled)[:,1]\n",
        "y_pred_L1\n",
        "\n",
        "print(y_pred_proba_L1[:5])\n",
        "print(y_pred_L1[:5])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X8-GnF9JJB3n"
      },
      "outputs": [],
      "source": [
        "# Probabilities of the test set being 0 and 1: RIDGE\n",
        "y_pred_proba_L2 = Log_Reg_L2.predict_proba(X_test_scaled)[:,1]\n",
        "y_pred_L2\n",
        "\n",
        "print(y_pred_proba_L2[:5])\n",
        "print(y_pred_L2[:5])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YcsC9sxpJB3o"
      },
      "source": [
        "### 5.4.2 Determining the fpr and tpr at each threshold value for logistic regression using Lasso (L1 Regularization)\n",
        "Now that we have the probabilities associated with each prediction, we know exactly which records are predicted YES and NO for each choice of decision threshold. Hence, we can determine the false positive rate (fpr) and true positive rate (tpr) for threshold value."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U2P3QTlqJB3p"
      },
      "outputs": [],
      "source": [
        "from sklearn import metrics\n",
        "y_test_binary = [0 if x=='no' else 1 for x in y_test]\n",
        "fpr, tpr,thresholds= metrics.roc_curve(y_true = y_test_binary, y_score = y_pred_proba_L1)\n",
        "\n",
        "# Defining dataframe with fpr and tpr at each threshold value for logistic regression using Lasso\n",
        "df = pd.DataFrame()\n",
        "df['fpr'] = fpr\n",
        "df['tpr'] = tpr\n",
        "\n",
        "# Check\n",
        "print(thresholds[:5])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DfKElB-FJB3q"
      },
      "source": [
        "### 5.4.3 Plotting the ROC curve"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zHJTvRhSJB3r"
      },
      "outputs": [],
      "source": [
        "import plotnine as p9\n",
        "import warnings; warnings.simplefilter('ignore')\n",
        "\n",
        "p = p9.ggplot(mapping = p9.aes(x = fpr, y = tpr), data = df)\n",
        "p += p9.geom_line(color = 'red')\n",
        "p += p9.geom_abline(p9.aes(intercept=0, slope=1), linetype = 'dashed', colour = 'blue')\n",
        "p += p9.labs(title = 'ROC Curve', x = 'fpr', y = 'tpr')\n",
        "p += p9.theme_bw()\n",
        "\n",
        "print(p)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j9mPezTlJB3s"
      },
      "source": [
        "### 5.4.4 Area Under the ROC curve (AUC)\n",
        "Note that AUC = 0.5 corresponds to random assignment."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PuEV_tObJB3t"
      },
      "outputs": [],
      "source": [
        "print(round(metrics.roc_auc_score(y_true = y_test_binary, y_score = y_pred_proba_L1),3))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vqmP4weUJB3u"
      },
      "source": [
        "<div class=\"alert alert-block alert-success\">**Start Activity 7**</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eqKHzUNCJB3u"
      },
      "source": [
        "### <font color='brown'> Question 7: Repeat this process for the logistic regression model using Ridge (L2-norm regularization). Explain AUC metric in your own words </font>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pALLOLxqJB3u"
      },
      "outputs": [],
      "source": [
        "# Write Python code here. TIP: In this case we need to use 'y_pred_proba_L2'.\n",
        "\n",
        "# Defining dataframe with fpr and tpr at each threshold value for logistic regression using Lasso\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0dwj0HR2JB3u"
      },
      "outputs": [],
      "source": [
        "# Write Python code here:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VK4TtgKtJB3u"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OrMogy5HJB3v"
      },
      "source": [
        "<b> Write the answer here:</b>\n",
        "#####################################################################################################################\n",
        "\n",
        "(Double-click here)\n",
        "\n",
        "\n",
        "#####################################################################################################################"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RYDVxn2eJB3v"
      },
      "source": [
        "<div class=\"alert alert-block alert-warning\">**End Activity 7**</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s6k_onlmJB3v"
      },
      "source": [
        "## 5.5 Explanation of how our models make predictions: Lime and Shap\n",
        "\n",
        "### **Let’s first use Lime to interpret the predictions from the logistic regression model with L1-norm (Lasso)**\n",
        "\n",
        "We will analyse how our logistic regression model with Lasso predicts 'readmission' with one patient. For this, we will assume that the first patient from the 'X_test' dataset is 'new' and that the model has not seen it in the training sample and has not been used in the test sample. Let's analyse this particular prediction using Lime."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RbgVKTUKJB3v"
      },
      "outputs": [],
      "source": [
        "import lime\n",
        "import lime.lime_tabular\n",
        "import shap                \n",
        "import time\n",
        "\n",
        "# Adding the explainer for Lime algorithm. More information: https://lime-ml.readthedocs.io/en/latest/lime.html\n",
        "explainer = lime.lime_tabular.LimeTabularExplainer(X_train_scaled,\n",
        "                                                   feature_names=X_train.columns.values.tolist(),\n",
        "                                                   class_names=np.unique(y_train))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P5ULDt3IJB3w"
      },
      "outputs": [],
      "source": [
        "# Get the explanation for our logistic regression model using Lasso\n",
        "exp = explainer.explain_instance(X_test_scaled[0], Log_Reg_L1.predict_proba, num_features=10)\n",
        "exp.show_in_notebook(show_all=False, show_table=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZH103vcaJB3w"
      },
      "source": [
        "### **Let’s use Lime to interpret the predictions from the logistic regression model with L2-norm (Ridge).**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q53MZ1QxJB3w"
      },
      "outputs": [],
      "source": [
        "# Get the explanation for our logistic regression model using Ridge\n",
        "exp = explainer.explain_instance(X_test_scaled[0], Log_Reg_L2.predict_proba, num_features=10)\n",
        "exp.show_in_notebook(show_all=False, show_table=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ySGn2C50JB3w"
      },
      "source": [
        "**Let's use SHAP explainer to compare results.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-dafgKbdJB3w"
      },
      "source": [
        "<div class=\"alert alert-block alert-success\">**Start Activity 8**</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hNzRzFdUJB3x"
      },
      "source": [
        "### <font color='brown'> Question 8: Could you use SHAP summary plot to visualise both models in a global sense in order to summarise the effect of all the features in the 'X_train_scaled' dataset?. </font> \n",
        "\n",
        "<p><font color='green'> Tip:  Please consider that we need to consider X_train_scaled for both models.</font></p>\n",
        "\n",
        "<font color='green'> Explainer for Logistic Regression: `shap.LinearExplainer(ModelName, X_train_scaled, feature_dependence='independent')`. \n",
        "\n",
        "<font color='green'> Getting shap value `LinearExplainer.shap_values(X_train_scaled)`\n",
        "\n",
        "<font color='green'> Summary plot `shap.summary_plot(shap_value, X_train_scaled, feature_names = X_train.columns)`\n",
        "\n",
        "<font color = 'green'> [SHAP reference](https://shap.readthedocs.io/en/latest/api.html)\n",
        "\n",
        "<p><font color='green'> NB: It could take 20-30 minutes to run."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OC-zQbQdJB3x"
      },
      "outputs": [],
      "source": [
        "# Write your Python code here (LASSO):\n",
        "\n",
        "# Using SHAP to explain predictions\n",
        "explainer_Model1 = \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vFHIiVHnJB3x"
      },
      "outputs": [],
      "source": [
        "# Summary plot - SHAP\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GYkN7w9-JB3x"
      },
      "outputs": [],
      "source": [
        "# Write your Python code here (RIDGE):\n",
        "\n",
        "# Using SHAP to explain predictions\n",
        "explainer_Model2 = "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4WmDzMCrJB3y"
      },
      "outputs": [],
      "source": [
        "# Summary plot - SHAP\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gcXKnOZUJB3y"
      },
      "source": [
        "### <font color='brown'> Question 9: Please briefly explain the results of the SHAP summary plot of Model 2 (Logistic Regression using Ridge). </font>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c_8daWuGJB3z"
      },
      "source": [
        "<b> Write the answer here:</b>\n",
        "#####################################################################################################################\n",
        "\n",
        "(Double-click here)\n",
        "\n",
        "\n",
        "#####################################################################################################################"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xLJJiYmGJB3z"
      },
      "source": [
        "<div class=\"alert alert-block alert-warning\">**End Activity 8**</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KJqzM1Gfkv2u"
      },
      "source": [
        "© 2022 Copyright The University of New South Wales - CRICOS 00098G"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "Week-02-Exercise02.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}